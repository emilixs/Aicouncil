import { LLMMessage, LLMConfig, LLMResponse } from '../dto';

/**
 * Abstract LLM Driver Interface
 * 
 * All driver implementations must extend this abstract class.
 * This enforces a consistent interface across all LLM providers,
 * enabling the Council orchestrator to work with any driver implementation
 * without knowing provider-specific details.
 */
export abstract class LLMDriver {
  /**
   * API key for the LLM provider
   */
  protected readonly apiKey: string;

  /**
   * Constructor
   * @param apiKey - API key for the LLM provider
   */
  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  /**
   * Non-streaming chat completion
   * 
   * @param messages - Array of conversation messages
   * @param config - Driver-specific configuration
   * @returns Promise resolving to the LLM response
   */
  abstract chat(messages: LLMMessage[], config: LLMConfig): Promise<LLMResponse>;

  /**
   * Streaming chat completion
   * 
   * Yields text chunks as they are generated by the LLM.
   * This allows seamless integration with WebSocket gateways in future phases.
   * 
   * @param messages - Array of conversation messages
   * @param config - Driver-specific configuration
   * @returns AsyncGenerator yielding text chunks
   */
  abstract streamChat(
    messages: LLMMessage[],
    config: LLMConfig,
  ): AsyncGenerator<string, void, unknown>;
}

